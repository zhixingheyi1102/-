# 支持向量机

## 硬间隔SVM

### 模型定义

​		最初SVM是为了解决二分类问题，使用$ w^Tx+b $ 的超平面(超过2维的切割平面）将两类数据分隔。但如果仅是将数据分隔开，其实有多个超平面都可以做到，那么哪一个超平面才是最好的呢？对比下图两个超平面，一个距离数据点过近，若在测试时，有圆圈数据稍微向左偏一点可能会被分类到×数据，因此这种超平面的泛化能力不好：

![img](https://pic3.zhimg.com/80/v2-2a1ab6f58f57a24a21416cfae428d9e2_720w.webp)

​		因此好的超平面，从距离的角度来看，即数据点离超平面越远越好。因此，我们若令距离超平面最近的点的距离尽可能大，即间隔最大化，那么所有点的预测结果确信度便是最高的。

​		下面即将这一思想用数学表示：
$$
目标：&\max\quad margin(w,b)\\
条件：&s.t.\quad y_i(w^Tx+b)>0\\
$$
​		其中：![image-20221220120852508](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220120852508.png)

<img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220140116921.png" alt="image-20221220140116921" style="zoom:60%;" />

因此，该优化问题可以表示为：
$$
目标：&\max_{w,b} \quad \min_i \frac{1}{\Vert w\Vert}\cdot|w^T\cdot x_i+b|=\max_{w,b} \quad \min_i \frac{1}{\Vert w\Vert}\cdot y_i(w^T\cdot x_i+b)=\max_{w,b} \quad \frac{1}{\Vert w\Vert}\\
&\min_i y_i(w^T\cdot x_i+b)\\
约束：&y_i(w^T\cdot x_i+b)>0
$$

> 在这里我们可以看到，一定存在一个参数 $ \gamma $使得 $ \min_i y_i(w^T\cdot x_i+b)=\gamma  $,而且该函数求出的间隔为函数间隔，也即当参数变为2$ w $,2$ b$时，虽然函数不发生变换，但间隔却会变大，因此我们应该对该式添加一个约束，及令$ \gamma$=1,此时得到的间隔就变成几何间隔。而且$ max_{w,b} \quad \frac{1}{\Vert w\Vert} 等价于 min \quad \frac{1}{2}w^Tw （为了计算方便）$ 

因此该优化问题可以表示为：
$$
目标：&min \quad \frac{1}{2}w^Tw  \\
约束：&y_i(w^T\cdot x_i+b)\ge1,i=1,2,\cdots,N
$$
因此这是一个有N个约束条件的凸优化问题。

### 模型求解

#### 拉普拉斯乘子法

​		拉普拉斯乘子法的目的实际是将一个有约束的优化问题中的优化函数和约束条件打包到一个函数里，将有约束优化问题转化为一个无约束的优化问题，公式如下：
$$
\begin{align}
优化函数：&R(x,y)=x^2e^yy\\
约束：&B(x,y)=x^2+y^2=b\\
&L(x,y,\lambda)=R(x,y)-\lambda(B(x,y)-b)\\
\end{align}
$$
<img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220114000123.png" alt="image-20221220114000123" style="zoom:50%;" />



​		下面说明为何这是一个打包函数：对于R函数来说，其所能求到的极值在与B函数恰好相切的位置，此时二者梯度大小相同，方向成比例，也即  $ \nabla R= \lambda \nabla B$ ，若有多个约束时，R函数的梯度等于多个约束函数的梯度向量和

<img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220153127685.png" alt="image-20221220153127685" style="zoom:50%;" align=center /><img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220113859204.png" alt="image-20221220113859204" style="zoom: 57%;" align=left />



​		此时我们对L函数求偏导，令其梯度为0,可以看到此时得到的点既满足约束条件，也满足上述所说的梯度成比例
$$
\nabla L =0\\
\frac{\partial h}{\partial x}=\frac{\partial R}{\partial x}-\lambda\frac{\partial B}{\partial x}=0\\
\frac{\partial h}{\partial y}=\frac{\partial R}{\partial y}-\lambda\frac{\partial B}{\partial y}=0\\
\frac{\partial h}{\partial \lambda}=B(x,y)-b=0
$$
​		使用这个方法的优势在于，我们在求最值时通常只有求导求梯度这一种方法，但是当加了约束条件时，很可能就不存在极值点、最值点。那么通过这一转换就使得该问题仍能使用求导求梯度来求解。

#### 将有约束问题转为无约束问题

​		因此我们就利用拉格朗日乘子法，将第一节得到的有约束模型转为无约束模型，如下所示：
$$
L(w,b,\lambda)= \frac{1}{2}w^Tw +\Sigma_{i=1}^N\lambda_i(1-y_i(w^Tx_i+b))\\
\min_{w,b} \max_\lambda L(w,b,\lambda)\\
s.t.\quad\lambda_i\ge0
$$

> 这里当$\lambda_i $>0时，其对应的约束条件起作用，为紧致的，当为0时，约束函数不起作用，为松弛的
>
> <img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220154143793.png" alt="image-20221220154143793" style="zoom:50%;" />

#### 拉格朗日对偶问题

​		刚才我们使用拉格朗日乘子法将有约束问题转为一个无约束问题，但此时又出现一个问题就是，如果这个L函数不是凸函数的话，其梯度为0的点就不一定是最值点。为了解决这个问题，就需要将该问题转为它的对偶问题：![image-20221220155957344](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220155957344.png)

>得到的对偶函数一定是一个凸函数，因为该函数与其参数$ \lambda、v$是一阶线性的关系，也就是一条直线，直线就是凸函数<img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220160211954.png" alt="image-20221220160211954" style="zoom:50%;" />

​		但是能将原问题转为对偶问题，并不意味着二者等价，因为$ \min_\limits x\quad\max_\limits{\lambda,v}L(x,\lambda,v)\ge \max_\limits{\lambda,v} g(\lambda,v)$，当二者为大于关系时，称为弱对偶关系，当二者相等时，称为强对偶关系，此时二者等价。要满足强对偶关系，就需要使x的可行域为一个凸集（是一个凸优化问题）。若二者满足强对偶关系，就可以推出其必然满足KKT条件：

<img src="C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221220162747650.png" alt="image-20221220162747650" style="zoom:50%;" align=center />

#### 将无约束问题转为对偶问题

将原问题转为其对偶问题，也就是：
$$
原问题:&\min_{w,b} \max_\lambda L(w,b,\lambda)\\
&s.t.\quad\lambda_i\ge0\\
对偶问题:& \max_\lambda \quad \min_{w,b}L(w,b,\lambda)\\
&s.t.\quad\lambda_i\ge0\\
$$
接下来就通过求梯度法对该对偶问题进行求解,得到参数w，也即得到该线性函数的斜率：
$$
\frac {\partial L}{\partial b}=0\rightarrow\Sigma_{i=1}^N\lambda_iy_i=0\\
\frac {\partial L}{\partial w}=0\rightarrow w=\Sigma_{i=1}^N\lambda_iy_ix_i
$$
再根据kkt条件中的互补松弛条件$ \lambda_i(1-y_i(w^Tx_i+b))$来求解b,根据互补松弛条件我们可以看到，只有当$w^Tx_i+b=\pm 1$的时候$ \lambda_i$才能不为0，从图上直观地来看，也就是说只有离超平面最近的点才会对超平面的选取造成影响：

![img](https://pic4.zhimg.com/80/v2-3fb320f5881e859fc60df5814cbff0b7_720w.webp)

可求得：
$$
\begin{align*}
b&=y_k-w^Tx\\
&=y_k-\Sigma_{i=1}^N\lambda_iy_ix_i^Tx_k\\
\end{align*}
$$
由此就求得了最优的超平面$w^Tx_i+b$。

## 软间隔SVM

### 模型定义

​		对硬间隔SVM来说，实际上其蕴含了一个假设，也就是数据需要线性可分，且两个数据分类之间的间隔要足够大，效果才好。但实际上数据通常会包含一些

噪声，或者数据本身就不是线性可分的，那效果就会变差，此时就要引入软间隔SVM，其核心思想就是添加一个损失函数，允许一点错误，将优化问题改造如下：
$$
目标：&min \quad \frac{1}{2}w^Tw +Loss \\
约束：&y_i(w^T\cdot x_i+b)\ge1,i=1,2,\cdots,N
$$
​		对于Loss函数的定义，有两种方式，第一种是用分类错误的样本数量来表示，但这样表示的Loss函数是不连续的，跳跃的，不具有良好的数学性质。因此我们使用第二种定义方式，也就是用每个点与超平面的距离来表示：若该数据点分类正确且其与超平面的距离大于超平面所要求的最小距离，就没有损失，反之有损失。用数学公式表示如下：
$$
\begin{align}
y_i(w^T\cdot x_i+b)\ge1 &\rightarrow Loss=0\\
y_i(w^T\cdot x_i+b)<1 &\rightarrow Loss=1-y_i(w^T\cdot x_i+b)
\end{align}
$$
​		因此，可将目标函数写为：
$$
目标：&min \quad \frac{1}{2}w^Tw +Cmax\{0,1-y_i(w^T\cdot x_i+b)\} \\
约束：&y_i(w^T\cdot x_i+b)\ge1,i=1,2,\cdots,N
$$
​		在这里，我们令$ \xi_i=1-y_i(w^T\cdot x_i+b),\xi\ge0$,可将目标函数简化为：
$$
目标：&min \quad \frac{1}{2}w^Tw +C\Sigma_{i=1}^N\xi_i \\
约束：&y_i(w^T\cdot x_i+b)\ge1-\xi_i,i=1,2,\cdots,N\\
&\xi_i\ge0
$$
​		实际上这个$ \xi_i$就是允许超平面上下移动的程度，原来需要保证超平面与点的距离比1大，而现在只需要保证比$1-\xi_i$大

### 模型求解

与硬间隔相似

## 核方法SVM

​		我们之前使用的方法都只适用于数据线性可分的情况，但是现实中大部分数据都是非线性可分的，那么在这种情况下就需要使用新的方法。目前对于这一问题的解决有两条理路，一条是从感知机算法到神经网络到深度学习，这一理路的想法是处理目标函数，通过不断地非线性变换，逐步去逼近那条分类的非线性函数；而另一条是从SVM到核方法SVM，它的想法是对数据做处理，通过非线性变换将数据升到高维空间，使其更加线性可分，但升维同时也会带来一个困难，就是数据变得可分的同时也更难聚类。

### 核函数

#### 引入

在上一节中，我们利用拉格朗日乘子法和对偶性最终改造成的优化问题如下所示：

![image-20221221130912775](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221221130912775.png)

其中有$ x_i^Tx_j$，是关于数据本身的内积，若对其使用非线性变换后，则为 $ \phi(x_i^T)\phi(x_j)$，很多时候我们对其变换后维度非常大，这样的计算量过大很影响效率，因此我们引入Kernel Function:

![image-20221221131104595](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221221131104595.png)

#### 定义

​		通常我们定义的核函数为正定核函数，定义如下：

![image-20221221131308408](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221221131308408.png)

#### 性质

​		正定核函数具有对称性和正定性：

![image-20221221131340370](C:\Users\goseedou\AppData\Roaming\Typora\typora-user-images\image-20221221131340370.png)













